{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Joint models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Part I: Binary Classifier"
      ],
      "metadata": {
        "id": "7MRAosS3q65p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxNZUMwgynV6",
        "outputId": "0e9c2648-c2e5-412b-d48f-57c4b3761052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Collecting tensorflow<2.10,>=2.9.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.46.3)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep_vNKUJr7q4",
        "outputId": "5be3d6be-1882-4d5e-e5b7-1b12aceeb14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 38.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import os, os.path\n",
        "import csv"
      ],
      "metadata": {
        "id": "QSs8TA6FyqUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9HgrsIx0Y86",
        "outputId": "702c9678-2db7-4190-b5de-e35f2c870991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_PATH = \"/content/drive/Shareddrives/CRP/\""
      ],
      "metadata": {
        "id": "Q6NDFFSBrnSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load binary classifier model\n",
        "y_reloaded_model = tf.keras.models.load_model((ROOT_PATH+'binary_model_350.h5'),custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "metadata": {
        "id": "ZEhfu9HryxUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Establish the directory of the folder with the files and the number of files the folder has\n",
        "DIR = ROOT_PATH+'PerfCRP'\n",
        "i=len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
        "j=i"
      ],
      "metadata": {
        "id": "QKLIlzNyV2oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load each text file of the folder in a row in a dataframe\n",
        "text = pd.read_csv(DIR+'/'+str(0)+'.txt', sep='\\n', header=None,quoting=csv.QUOTE_NONE)[0].str.cat()\n",
        "df = pd.DataFrame([text], columns=['text'])\n",
        "for i in range(j):\n",
        "  text = pd.read_csv(DIR+'/'+str(i)+'.txt', sep='\\n', header=None,quoting=csv.QUOTE_NONE)[0].str.cat()\n",
        "  dfs = pd.DataFrame([text], columns=['text'])\n",
        "  df=df.append(dfs)\n",
        "df.index = range(len(df))\n",
        "df=df.drop(0)\n",
        "df.index = range(len(df))"
      ],
      "metadata": {
        "id": "IPkGw1zGVa8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply model to dataframe/files\n",
        "output=y_reloaded_model.predict(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_fTmOgk4Qf0",
        "outputId": "2437f7c2-344b-42a9-f895-370c73c163f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 20s 5s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop those who have less than a 50% chance according to the model of being a biotech review\n",
        "for i in range(j):\n",
        "  if output[i][0]<0.50:\n",
        "    df=df.drop(i)\n",
        "df.index = range(len(df))"
      ],
      "metadata": {
        "id": "9S9a4HtQLgHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final dataframe to use for other model\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "RlC4xeOfLwNi",
        "outputId": "2873e0d4-d9cb-4503-854e-6bfafce158b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text\n",
              "0   Biocatalysis expert Enzymaster (Germany, China...\n",
              "1   Andera Partners, Paris based European private ...\n",
              "2   ransine Therapeutics Ltd has added £4.6m from ...\n",
              "3   Minoryx Therapeutics SL has closed a €51m fina...\n",
              "4   Avantium NV will get a €3m EU funding for a fe...\n",
              "5   Cancer specialist Engimmune Therapeutics has r...\n",
              "6   Italian start-up Enthera Pharmaceuticals has r...\n",
              "7   Oxford-based Adaptimmune Therapeutics plc has ...\n",
              "8   Oslo-based microbiome expert Bio-Me A/S has ra...\n",
              "9   Swedish research-based late-stage biotech Cinc...\n",
              "10  Lithuanian biotech Droplet Genomics has raised...\n",
              "11  Copenhagen-based biotech company MonTa BioScie...\n",
              "12  Co-founded by Ingvar Helgason and stem cell sc...\n",
              "13  Finnish startup FinnAdvance has recently raise...\n",
              "14  BioGenCell raises $16 million Seed round led b...\n",
              "15  Biotech Company Polypid Raises $15 MillionIsra...\n",
              "16  Hong Kong-based biotech startup Insilico Medic..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0264ca14-772d-4b3b-9394-e1201e95c15f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Biocatalysis expert Enzymaster (Germany, China...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Andera Partners, Paris based European private ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ransine Therapeutics Ltd has added £4.6m from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Minoryx Therapeutics SL has closed a €51m fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Avantium NV will get a €3m EU funding for a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Cancer specialist Engimmune Therapeutics has r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Italian start-up Enthera Pharmaceuticals has r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Oxford-based Adaptimmune Therapeutics plc has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Oslo-based microbiome expert Bio-Me A/S has ra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Swedish research-based late-stage biotech Cinc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Lithuanian biotech Droplet Genomics has raised...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Copenhagen-based biotech company MonTa BioScie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Co-founded by Ingvar Helgason and stem cell sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Finnish startup FinnAdvance has recently raise...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>BioGenCell raises $16 million Seed round led b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Biotech Company Polypid Raises $15 MillionIsra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Hong Kong-based biotech startup Insilico Medic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0264ca14-772d-4b3b-9394-e1201e95c15f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0264ca14-772d-4b3b-9394-e1201e95c15f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0264ca14-772d-4b3b-9394-e1201e95c15f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part II: NER"
      ],
      "metadata": {
        "id": "Pfx5Tv7uq-r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the NER task, we use **BertForTokenClassification** which is included in the **Transformers library** by **HuggingFace**."
      ],
      "metadata": {
        "id": "ZHgKN22erKSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnxEg1HosHaa",
        "outputId": "68cfa132-46d3-4c4c-f8fd-e06588410ff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input a list of news articles for NER\n",
        "# suppose the name of the file is 'news.csv', with columns including 'Source', 'Title', 'Date', 'Content', 'URL'\n",
        "# news_articles = pd.read_csv('news.csv')\n",
        "# here we have the datafram df from binary classifier\n",
        "news_articles = df"
      ],
      "metadata": {
        "id": "FLh9WShAZ5MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "tokenizer = BertTokenizerFast.from_pretrained(ROOT_PATH+'ner_trained_model')\n",
        "#ID - Label conversion\n",
        "labels_to_ids = {'O': 0, 'B-s': 1, 'B-p': 2, 'B-i': 3, 'B-f': 4, 'B-y': 5, 'I-s': 6, 'I-p': 7, 'I-i': 8, 'I-f': 9, 'I-y': 10}\n",
        "ids_to_labels = {0:'O', 1:'B-s', 2:'B-p', 3:'B-i', 4:'B-f', 5:'B-y', 6:'I-s', 7:'I-p', 8:'I-i', 9:'I-f', 10:'I-y'}"
      ],
      "metadata": {
        "id": "VKsK0CMKtD-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForTokenClassification.from_pretrained(ROOT_PATH+'ner_trained_model', num_labels=len(labels_to_ids))\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP9-MC-qrOiX",
        "outputId": "eb9ce117-a206-4511-8d33-80711672134d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for returning list of recognized labels for each article\n",
        "def inference_ner(news_article):\n",
        "  sentence = news_article\n",
        "\n",
        "  inputs = tokenizer(sentence.split(),\n",
        "                      is_split_into_words=True, \n",
        "                      return_offsets_mapping=True, \n",
        "                      padding='max_length', \n",
        "                      truncation=True, \n",
        "                      max_length=MAX_LEN,\n",
        "                      return_tensors=\"pt\")\n",
        "\n",
        "  # move to gpu\n",
        "  ids = inputs[\"input_ids\"].to(device)\n",
        "  mask = inputs[\"attention_mask\"].to(device)\n",
        "  # forward pass\n",
        "  outputs = model(ids, attention_mask=mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "  tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "  token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
        "  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "  prediction = []\n",
        "  for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
        "    #only predictions on first word pieces are important\n",
        "    if mapping[0] == 0 and mapping[1] != 0:\n",
        "      prediction.append(token_pred[1])\n",
        "    else:\n",
        "      continue\n",
        "  return(prediction)"
      ],
      "metadata": {
        "id": "X8-t6GvCrO3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference for all the input articles\n",
        "# inference_result = pd.DataFrame(columns = ['Startup', 'Founder(s)', 'Investor(s)', 'City/Country', 'Year Founded', 'Source', 'Title', 'Date', 'Content', 'URL'])\n",
        "inference_result = pd.DataFrame(columns = ['Startup', 'Founder(s)', 'Investor(s)', 'City/Country', 'Year Founded', 'News Content'])\n",
        "for _, news_article in news_articles.iterrows(): \n",
        "  prediction = inference_ner(news_article['text'])\n",
        "  news_article_split = news_article['text'].split()\n",
        "  try:\n",
        "    startup = set([news_article_split[i].strip().upper().translate(str.maketrans('', '', string.punctuation)) for i in [index for (index, item) in enumerate(prediction) if item == 'B-s' or item == 'I-s']])\n",
        "    if len(startup) == 0:\n",
        "      startup = np.NaN\n",
        "  except:\n",
        "    startup = np.NaN\n",
        "  try:\n",
        "    founder = set([news_article_split[i].strip().upper().translate(str.maketrans('', '', string.punctuation)) for i in [index for (index, item) in enumerate(prediction) if item == 'B-f' or item == 'I-f']])\n",
        "    if len(founder) == 0:\n",
        "      founder = np.NaN\n",
        "  except:\n",
        "    founder = np.NaN\n",
        "  try:\n",
        "    investor = set([news_article_split[i].strip().upper().translate(str.maketrans('', '', string.punctuation)) for i in [index for (index, item) in enumerate(prediction) if item == 'B-i' or item == 'I-i']])\n",
        "    if len(investor) == 0:\n",
        "      investor = np.NaN\n",
        "  except:\n",
        "    investor = np.NaN\n",
        "  try:\n",
        "    place = set([news_article_split[i].strip().upper().replace('BASED', '').replace('HEADQUARTRED', '').translate(str.maketrans('', '', string.punctuation)) for i in [index for (index, item) in enumerate(prediction) if item == 'B-p' or item == 'I-p']])\n",
        "    if len(place) == 0:\n",
        "      place = np.NaN   \n",
        "  except:\n",
        "    place = np.NaN\n",
        "  try:\n",
        "    year_founded = set([news_article_split[i].strip().upper().translate(str.maketrans('', '', string.punctuation)) for i in [index for (index, item) in enumerate(prediction) if item == 'B-y' or item == 'I-y']])\n",
        "    if len(year_founded) == 0:\n",
        "      year_founded = np.NaN\n",
        "  except:\n",
        "    year_founded = np.NaN\n",
        "  inference_result = inference_result.append({\n",
        "    'Startup': startup, \n",
        "    'Founder(s)': founder, \n",
        "    'Investor(s)': investor, \n",
        "    'City/Country': place, \n",
        "    'Year Founded': year_founded, \n",
        "    #'Source': news_article['Source'], \n",
        "    #'Title': news_article['Title'], \n",
        "    #'Date': news_article['Date'], \n",
        "    'News Content': news_article['text']#, \n",
        "    #'URL': news_article['URL']\n",
        "    }, ignore_index=True)"
      ],
      "metadata": {
        "id": "iVn1orr5rSmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "zgDH636mtouh",
        "outputId": "fddc5a64-dcf7-439f-88d9-560d4762f4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              Startup  \\\n",
              "0                        {ENZYMASTER}   \n",
              "1           {THERAPEUTICS, MINERALYS}   \n",
              "2   {TRANSINE, THERAPEUTICS, RANSINE}   \n",
              "3             {MINORYX, THERAPEUTICS}   \n",
              "4              {WATERPROOF, AVANTIUM}   \n",
              "5           {THERAPEUTICS, ENGIMMUNE}   \n",
              "6          {PHARMACEUTICALS, ENTHERA}   \n",
              "7         {THERAPEUTICS, ADAPTIMMUNE}   \n",
              "8                     {BIOME, EXPERT}   \n",
              "9                   {CINCLUS, PHARMA}   \n",
              "10                {GENOMICS, DROPLET}   \n",
              "11          {MONTA, APS, BIOSCIENCES}   \n",
              "12                        {VITROLABS}   \n",
              "13                      {FINNADVANCE}   \n",
              "14                       {BIOGENCELL}   \n",
              "15                          {POLYPID}   \n",
              "16               {MEDICINE, INSILICO}   \n",
              "\n",
              "                                 Founder(s)  \\\n",
              "0                                       NaN   \n",
              "1                                       NaN   \n",
              "2                              {THIRKETTLE}   \n",
              "3                                       NaN   \n",
              "4                                       NaN   \n",
              "5                                       NaN   \n",
              "6   {CAMBONI, SPINELLI, SILVANO, GABRIELLA}   \n",
              "7                                       NaN   \n",
              "8                                       NaN   \n",
              "9                                       NaN   \n",
              "10                                      NaN   \n",
              "11                                      NaN   \n",
              "12      {INGVAR, HELGASON, DUSKO, ILIC, DR}   \n",
              "13                                      NaN   \n",
              "14                                      NaN   \n",
              "15                                      NaN   \n",
              "16                                      NaN   \n",
              "\n",
              "                                          Investor(s)           City/Country  \\\n",
              "0   {BAYLAND, PUHUA, CAPITAL, CARBONNEUTRAL, YUANB...                    NaN   \n",
              "1         {CAPITAL, ANDERA, MANAGEMENT, RA, PARTNERS}  {PARIS, PHILADELPHIA}   \n",
              "2   {CAPITAL, DISCOVERY, FUND, VENTURES, EPIDAREX,...                    NaN   \n",
              "3   {CAPITAL, CDTI, COLUMBUS, CAIXA, RISC, INNVIER...            {BARCELONA}   \n",
              "4                                                 NaN                    NaN   \n",
              "5                 {BIOVENTURES, NOVO, AREA, HOLDINGS}                    NaN   \n",
              "6             {INC, SOFINNOVA, ABBVIE, PARTNERS, T1D}              {ITALIAN}   \n",
              "7   {ENTERPRISE, CAPITAL, NEW, PORTFOLIO, MANAGEME...               {OXFORD}   \n",
              "8                                                 NaN                 {OSLO}   \n",
              "9   {AB, IRRUS, EIR, I, NOMINEE, APFONDEN, INVESTM...                    NaN   \n",
              "10                                {CAPITAL, PRACTICA}                    NaN   \n",
              "11                                                NaN   {COPENHAGEN, DANISH}   \n",
              "12  {BESTSELLER’S, DICAPRIO, FWD, NEW, REGENERATIO...               {LONDON}   \n",
              "13  {ICEBREAKER, TAKOA, VENTURES, ATHENSMED, INVES...                 {OULU}   \n",
              "14                                           {MARIUS}              {ISRAELI}   \n",
              "15                          {XENIA, CAPITAL, VENTURE}                    NaN   \n",
              "16         {CAPITAL, BHR, QIMING, PAVILION, PARTNERS}           {HONG, KONG}   \n",
              "\n",
              "   Year Founded                                       News Content  \n",
              "0           NaN  Biocatalysis expert Enzymaster (Germany, China...  \n",
              "1           NaN  Andera Partners, Paris based European private ...  \n",
              "2           NaN  ransine Therapeutics Ltd has added £4.6m from ...  \n",
              "3           NaN  Minoryx Therapeutics SL has closed a €51m fina...  \n",
              "4           NaN  Avantium NV will get a €3m EU funding for a fe...  \n",
              "5           NaN  Cancer specialist Engimmune Therapeutics has r...  \n",
              "6           NaN  Italian start-up Enthera Pharmaceuticals has r...  \n",
              "7           NaN  Oxford-based Adaptimmune Therapeutics plc has ...  \n",
              "8           NaN  Oslo-based microbiome expert Bio-Me A/S has ra...  \n",
              "9           NaN  Swedish research-based late-stage biotech Cinc...  \n",
              "10          NaN  Lithuanian biotech Droplet Genomics has raised...  \n",
              "11          NaN  Copenhagen-based biotech company MonTa BioScie...  \n",
              "12          NaN  Co-founded by Ingvar Helgason and stem cell sc...  \n",
              "13          NaN  Finnish startup FinnAdvance has recently raise...  \n",
              "14          NaN  BioGenCell raises $16 million Seed round led b...  \n",
              "15          NaN  Biotech Company Polypid Raises $15 MillionIsra...  \n",
              "16          NaN  Hong Kong-based biotech startup Insilico Medic...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-33826645-5ae7-4b95-bbe9-8f4c66319e1e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Startup</th>\n",
              "      <th>Founder(s)</th>\n",
              "      <th>Investor(s)</th>\n",
              "      <th>City/Country</th>\n",
              "      <th>Year Founded</th>\n",
              "      <th>News Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{ENZYMASTER}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{BAYLAND, PUHUA, CAPITAL, CARBONNEUTRAL, YUANB...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biocatalysis expert Enzymaster (Germany, China...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{THERAPEUTICS, MINERALYS}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{CAPITAL, ANDERA, MANAGEMENT, RA, PARTNERS}</td>\n",
              "      <td>{PARIS, PHILADELPHIA}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Andera Partners, Paris based European private ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{TRANSINE, THERAPEUTICS, RANSINE}</td>\n",
              "      <td>{THIRKETTLE}</td>\n",
              "      <td>{CAPITAL, DISCOVERY, FUND, VENTURES, EPIDAREX,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ransine Therapeutics Ltd has added £4.6m from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{MINORYX, THERAPEUTICS}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{CAPITAL, CDTI, COLUMBUS, CAIXA, RISC, INNVIER...</td>\n",
              "      <td>{BARCELONA}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Minoryx Therapeutics SL has closed a €51m fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{WATERPROOF, AVANTIUM}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Avantium NV will get a €3m EU funding for a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>{THERAPEUTICS, ENGIMMUNE}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{BIOVENTURES, NOVO, AREA, HOLDINGS}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cancer specialist Engimmune Therapeutics has r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>{PHARMACEUTICALS, ENTHERA}</td>\n",
              "      <td>{CAMBONI, SPINELLI, SILVANO, GABRIELLA}</td>\n",
              "      <td>{INC, SOFINNOVA, ABBVIE, PARTNERS, T1D}</td>\n",
              "      <td>{ITALIAN}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Italian start-up Enthera Pharmaceuticals has r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>{THERAPEUTICS, ADAPTIMMUNE}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{ENTERPRISE, CAPITAL, NEW, PORTFOLIO, MANAGEME...</td>\n",
              "      <td>{OXFORD}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Oxford-based Adaptimmune Therapeutics plc has ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>{BIOME, EXPERT}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{OSLO}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Oslo-based microbiome expert Bio-Me A/S has ra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>{CINCLUS, PHARMA}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{AB, IRRUS, EIR, I, NOMINEE, APFONDEN, INVESTM...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Swedish research-based late-stage biotech Cinc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>{GENOMICS, DROPLET}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{CAPITAL, PRACTICA}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lithuanian biotech Droplet Genomics has raised...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>{MONTA, APS, BIOSCIENCES}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{COPENHAGEN, DANISH}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Copenhagen-based biotech company MonTa BioScie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>{VITROLABS}</td>\n",
              "      <td>{INGVAR, HELGASON, DUSKO, ILIC, DR}</td>\n",
              "      <td>{BESTSELLER’S, DICAPRIO, FWD, NEW, REGENERATIO...</td>\n",
              "      <td>{LONDON}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Co-founded by Ingvar Helgason and stem cell sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>{FINNADVANCE}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{ICEBREAKER, TAKOA, VENTURES, ATHENSMED, INVES...</td>\n",
              "      <td>{OULU}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Finnish startup FinnAdvance has recently raise...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>{BIOGENCELL}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{MARIUS}</td>\n",
              "      <td>{ISRAELI}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>BioGenCell raises $16 million Seed round led b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>{POLYPID}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{XENIA, CAPITAL, VENTURE}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Biotech Company Polypid Raises $15 MillionIsra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>{MEDICINE, INSILICO}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>{CAPITAL, BHR, QIMING, PAVILION, PARTNERS}</td>\n",
              "      <td>{HONG, KONG}</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hong Kong-based biotech startup Insilico Medic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33826645-5ae7-4b95-bbe9-8f4c66319e1e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-33826645-5ae7-4b95-bbe9-8f4c66319e1e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-33826645-5ae7-4b95-bbe9-8f4c66319e1e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database = pd.read_csv(ROOT_PATH+'test_database.csv')\n",
        "nrow_before = database.shape[0]\n",
        "startup_list = []\n",
        "for items in database['Name'].iteritems():\n",
        "  startup_list.append(set(filter(None, [i.strip().upper().translate(str.maketrans('', '', string.punctuation)) for i in str(items[1]).split()])))"
      ],
      "metadata": {
        "id": "gAE0qf3azKkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the company exists in the database\n",
        "for _, i in inference_result.iterrows():\n",
        "  if i['Startup'] not in startup_list:\n",
        "    database.loc[len(database.index)] = [i['Startup'], i['Founder(s)'], i['City/Country'], i['Investor(s)'], i['Year Founded']]"
      ],
      "metadata": {
        "id": "PFB_5lrSrXiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nrow_after = database.shape[0]\n",
        "if nrow_after > nrow_before:\n",
        "  database.to_csv(ROOT_PATH+'test_database.csv', index=False)\n",
        "  print(str(nrow_after-nrow_before)+' startups have been added to the database!')\n",
        "else:\n",
        "  print('No startup is added to the database!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnXspgNx4_rC",
        "outputId": "a8290ee3-4339-4292-b634-77689b55f99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No startup is added to the database!\n"
          ]
        }
      ]
    }
  ]
}